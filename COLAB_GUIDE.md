# Google Colab KullanÄ±m KÄ±lavuzu

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### AdÄ±m 1: Projeyi GitHub'a YÃ¼kle

```bash
# Lokal bilgisayarÄ±nda
git init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/YOUR_USERNAME/lora-finetuning.git
git push -u origin main
```

### AdÄ±m 2: Google Colab'Ä± AÃ§

1. [colab.research.google.com](https://colab.research.google.com) adresine git
2. **File > Upload notebook** seÃ§
3. `colab_training_deep.ipynb` dosyasÄ±nÄ± yÃ¼kle

### AdÄ±m 3: GPU'yu Aktif Et

1. **Runtime > Change runtime type** tÄ±kla
2. **Hardware accelerator**: T4 GPU seÃ§
3. **Save** tÄ±kla

### AdÄ±m 4: Notebook'u Ã‡alÄ±ÅŸtÄ±r

Her hÃ¼creyi sÄ±rayla Ã§alÄ±ÅŸtÄ±r (Shift+Enter):

1. âœ… GPU kontrolÃ¼
2. âœ… Paketleri kur
3. âœ… Projeyi indir
4. âœ… Drive'Ä± baÄŸla
5. âœ… Config ayarla
6. âœ… Model yÃ¼kle
7. âœ… Training baÅŸlat (2-4 saat)
8. âœ… Model kaydet

## ğŸ“‹ DetaylÄ± AdÄ±mlar

### 1. GPU KontrolÃ¼

```python
!nvidia-smi
```

**Beklenen Ã§Ä±ktÄ±**: Tesla T4, 16GB VRAM

### 2. Paket Kurulumu

```python
!pip install -q torch transformers peft datasets accelerate bitsandbytes tqdm
```

**SÃ¼re**: ~2-3 dakika

### 3. Proje Ä°ndirme

```python
!git clone https://github.com/YOUR_USERNAME/lora-finetuning.git
%cd lora-finetuning
```

**Not**: `YOUR_USERNAME` yerine kendi GitHub kullanÄ±cÄ± adÄ±nÄ± yaz!

### 4. Google Drive BaÄŸlantÄ±sÄ±

```python
from google.colab import drive
drive.mount('/content/drive')
```

**Ä°zin ver**: Google hesabÄ±nÄ± seÃ§ ve izin ver

**Neden gerekli?**: 
- Checkpoint'ler Drive'a kaydedilir
- Oturum bitince silinmez
- Sonra indirebilirsin

### 5. Training

Training baÅŸladÄ±ÄŸÄ±nda:
- â±ï¸ **SÃ¼re**: 2-4 saat
- ğŸ“Š **Ä°lerleme**: Progress bar gÃ¶receksin
- ğŸ’¾ **Otomatik kayÄ±t**: Her 100 step'te checkpoint kaydedilir
- âš ï¸ **Oturumu aÃ§Ä±k tut**: TarayÄ±cÄ±yÄ± kapatma!

### 6. Training SÄ±rasÄ±nda Ä°zleme

```python
# Loss deÄŸerlerini gÃ¶rmek iÃ§in
# Her 20-40 step'te train loss
# Her 100-120 step'te eval loss
```

## ğŸ”„ DIVERSE Dataset iÃ§in

1. `colab_training_diverse.ipynb` dosyasÄ±nÄ± yÃ¼kle
2. AynÄ± adÄ±mlarÄ± tekrarla
3. **Ã–nemli**: DEEP training bittikten sonra baÅŸla!

## ğŸ’¾ Checkpoint'leri Ä°ndirme

### YÃ¶ntem 1: Drive'dan Ä°ndir

1. Google Drive'Ä± aÃ§
2. `MyDrive/lora_checkpoints/` klasÃ¶rÃ¼ne git
3. `deep/` veya `diverse/` klasÃ¶rÃ¼nÃ¼ indir

### YÃ¶ntem 2: Notebook'tan Ä°ndir

```python
# Log dosyalarÄ±nÄ± zip'le
!zip -r training_logs.zip /content/drive/MyDrive/lora_checkpoints/deep/logs

# Ä°ndir
from google.colab import files
files.download('training_logs.zip')
```

## âš ï¸ Ã–nemli Notlar

### Oturum Kesilirse

EÄŸer oturum kesilirse (12 saat veya 90 dakika boÅŸta):
1. âœ… **Checkpoint'ler Drive'da kayÄ±tlÄ±** - Kaybolmaz!
2. âœ… Training'e kaldÄ±ÄŸÄ± yerden devam edebilirsin
3. âœ… En son checkpoint'ten devam et

### Devam Etme Kodu

```python
# En son checkpoint'i bul
import os
checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]
latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))
resume_from = os.path.join(checkpoint_dir, latest_checkpoint)

# Training'e devam et
trainer.train(resume_from_checkpoint=resume_from)
```

### Memory SorunlarÄ±

EÄŸer OOM (Out of Memory) hatasÄ± alÄ±rsan:

```python
# Config'i gÃ¼ncelle
TrainingConfig.per_device_batch_size = 1  # Zaten 1
TrainingConfig.gradient_accumulation_steps = 32  # 16'dan artÄ±r
TrainingConfig.max_length_solution = 800  # 1024'ten dÃ¼ÅŸÃ¼r
```

## ğŸ“Š Training SonrasÄ±

### 1. Log'larÄ± Ä°ncele

```python
import json

log_file = '/content/drive/MyDrive/lora_checkpoints/deep/logs/training_log_*.jsonl'

# Log'larÄ± oku
with open(log_file, 'r') as f:
    logs = [json.loads(line) for line in f]

# Train loss'larÄ± gÃ¶ster
train_losses = [(log['step'], log['train_loss']) for log in logs if 'train_loss' in log]
print(train_losses)
```

### 2. Model Test Et

```python
# EÄŸitilmiÅŸ model ile test
test_problem = "Write a function to reverse a string"
prompt = f"You are an expert Python programmer. Please read the problem carefully before writing any Python code.\n\nProblem:\n{test_problem}\n\nSolution:\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=256)
solution = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(solution)
```

### 3. HuggingFace'e YÃ¼kle

```python
# HuggingFace login
from huggingface_hub import notebook_login
notebook_login()

# Model yÃ¼kle
model.push_to_hub("your-username/qwen-coder-lora-deep")
tokenizer.push_to_hub("your-username/qwen-coder-lora-deep")
```

## ğŸ¯ Checklist

### DEEP Training:
- [ ] Colab'da GPU aktif
- [ ] Notebook yÃ¼klendi
- [ ] GitHub repo linki gÃ¼ncellendi
- [ ] Drive baÄŸlandÄ±
- [ ] Training baÅŸladÄ±
- [ ] Training tamamlandÄ± (2-4 saat)
- [ ] Checkpoint'ler Drive'a kaydedildi
- [ ] Log'lar indirildi

### DIVERSE Training:
- [ ] DEEP training bitti
- [ ] Yeni Colab oturumu aÃ§Ä±ldÄ±
- [ ] GPU aktif
- [ ] `colab_training_diverse.ipynb` yÃ¼klendi
- [ ] Training baÅŸladÄ±
- [ ] Training tamamlandÄ±
- [ ] Checkpoint'ler kaydedildi

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Oturumu aÃ§Ä±k tut**: TarayÄ±cÄ± sekmesini kapatma
2. **Ä°nternet baÄŸlantÄ±sÄ±**: Stabil olmalÄ±
3. **Drive alanÄ±**: ~10-15GB boÅŸ alan gerekli
4. **Gece Ã§alÄ±ÅŸtÄ±r**: Uzun sÃ¼receÄŸi iÃ§in gece baÅŸlat
5. **Ä°ki training ayrÄ±**: DEEP ve DIVERSE'i ayrÄ± oturumlarda yap

## ğŸ†˜ Sorun Giderme

### "Runtime disconnected"
- Oturum kesildi, checkpoint'ten devam et
- Drive'daki checkpoint'ler kayÄ±tlÄ±

### "CUDA out of memory"
- Batch size'Ä± dÃ¼ÅŸÃ¼r (zaten 1)
- Context length'i dÃ¼ÅŸÃ¼r (800'e)
- Runtime'Ä± restart et

### "Dataset not found"
- Ä°nternet baÄŸlantÄ±sÄ±nÄ± kontrol et
- HuggingFace eriÅŸilebilir mi kontrol et

### "Module not found"
- Paketleri tekrar kur
- Proje dizinini kontrol et (`%cd lora-finetuning`)

## ğŸ“ YardÄ±m

Sorun yaÅŸarsan:
1. Hata mesajÄ±nÄ± kaydet
2. Hangi hÃ¼crede olduÄŸunu not et
3. GPU durumunu kontrol et (`!nvidia-smi`)
4. E-posta: arda.mulayim@outlook.com

---

**BaÅŸarÄ±lar!** ğŸš€
