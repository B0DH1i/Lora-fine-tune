# LoRA Fine-Tuning Projesi: Competitive Code Reasoning

Bu proje, Qwen2.5-Coder-1.5B-Instruct modelini LoRA kullanarak iki farklÄ± dataset ile fine-tune etmeyi amaÃ§lar.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# 1. Kurulum
pip install -r requirements.txt

# 2. TÃ¼m adÄ±mlarÄ± otomatik Ã§alÄ±ÅŸtÄ±r
python scripts/quick_start.py

# VEYA manuel olarak:

# 3. Ä°lk model testi
python scripts/inference_test.py

# 4. Dataset analizi
python scripts/dataset_analysis.py

# 5. Training
python scripts/train_deep.py
python scripts/train_diverse.py

# 6. DeÄŸerlendirme
python scripts/evaluate.py --base_dir ./checkpoints/deep --dataset deep
```

## ğŸ“ Proje YapÄ±sÄ±

```
â”œâ”€â”€ config/                      # KonfigÃ¼rasyon dosyalarÄ±
â”‚   â”œâ”€â”€ training_config.py      # Training hyperparameters
â”‚   â””â”€â”€ model_config.py          # Model ve LoRA ayarlarÄ±
â”œâ”€â”€ data/                        # Dataset iÅŸlemleri
â”‚   â”œâ”€â”€ dataset_loader.py        # Dataset yÃ¼kleme ve preprocessing
â”‚   â””â”€â”€ data_collator.py         # Batch hazÄ±rlama
â”œâ”€â”€ models/                      # Model yÃ¼kleme ve setup
â”‚   â”œâ”€â”€ model_loader.py          # Base model yÃ¼kleme
â”‚   â””â”€â”€ lora_setup.py            # LoRA konfigÃ¼rasyonu
â”œâ”€â”€ training/                    # Training loop
â”‚   â”œâ”€â”€ trainer.py               # Trainer setup
â”‚   â””â”€â”€ callbacks.py             # Logging ve early stopping
â”œâ”€â”€ evaluation/                  # DeÄŸerlendirme
â”‚   â”œâ”€â”€ evaluator.py             # Model deÄŸerlendirme
â”‚   â””â”€â”€ metrics.py               # Metrik hesaplama
â”œâ”€â”€ scripts/                     # Ã‡alÄ±ÅŸtÄ±rÄ±labilir script'ler
â”‚   â”œâ”€â”€ inference_test.py        # Ä°lk model testi (GÃ¶rev 1)
â”‚   â”œâ”€â”€ dataset_analysis.py      # Dataset analizi (GÃ¶rev 2)
â”‚   â”œâ”€â”€ train_deep.py            # DEEP training (GÃ¶rev 3)
â”‚   â”œâ”€â”€ train_diverse.py         # DIVERSE training (GÃ¶rev 3)
â”‚   â”œâ”€â”€ evaluate.py              # Checkpoint deÄŸerlendirme (GÃ¶rev 4)
â”‚   â””â”€â”€ quick_start.py           # TÃ¼m adÄ±mlarÄ± Ã§alÄ±ÅŸtÄ±r
â”œâ”€â”€ USAGE_GUIDE.md               # DetaylÄ± kullanÄ±m kÄ±lavuzu
â”œâ”€â”€ TROUBLESHOOTING.md           # Sorun giderme
â”œâ”€â”€ CHECKLIST.md                 # Teslim kontrol listesi
â”œâ”€â”€ MODEL_CARD_TEMPLATE.md       # HuggingFace model card ÅŸablonu
â””â”€â”€ requirements.txt             # Gerekli paketler
```



## âš™ï¸ KonfigÃ¼rasyon

### Training Hyperparameters
`config/training_config.py` dosyasÄ±nÄ± dÃ¼zenleyin:
- Learning rate: `2e-4`
- Batch size: `1` (gradient accumulation: `16`)
- Max epochs: `3`
- Context length: `1024` (solution) / `8192` (reasoning)

### LoRA KonfigÃ¼rasyonu
`config/model_config.py` dosyasÄ±nÄ± dÃ¼zenleyin:
- Rank (r): `32`
- Alpha: `64` (r * 2)
- Dropout: `0.1`
- Target modules: Attention + MLP layers




## ğŸ“Š Training LoglarÄ±

Loglar otomatik kaydedilir:
- Her 20-40 step: train loss
- Her 100-120 step: validation loss
- Konum: `checkpoints/[deep|diverse]/logs/`

## ğŸ”— Kaynaklar

- **Base Model**: [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)
- **DEEP Dataset**: [CodeGen-Deep5K](https://huggingface.co/datasets/Naholav/CodeGen-Deep5K)
- **DIVERSE Dataset**: [CodeGen-Diverse-5K](https://huggingface.co/datasets/Naholav/CodeGenDiverse-5K)
- **LoRA DokÃ¼mantasyonu**: [HuggingFace LoRA Guide](https://huggingface.co/docs/diffusers/training/lora)



## ğŸ“ Lisans

[Lisans bilgisi eklenecek]
